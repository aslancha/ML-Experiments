{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"LFI - A3.3 - Architecture 3 - Given Arch - Residual - 2x Conv Filters - Adam - Batch Norm - Kernel Size 5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1579782002295,"user_tz":-60,"elapsed":27481,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"id":"NS73LKRgJNuO","outputId":"2ce0cc27-40f6-47fe-a5d1-bc5d75d446bc","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# source code inspireed by\n","# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code\n","\n","from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import copy\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","\n","\n","\n","root = './data'\n","\n","transform = transforms.Compose([\n","    # you can add other transformations in this list\n","    transforms.ToTensor()\n","])\n","\n","train_set = datasets.FashionMNIST(root=root, train=True, transform=transform, download=True)\n","test_set = datasets.FashionMNIST(root=root, train=False, transform=transform, download=True)\n","\n","\n","# hyperparameter\n","# TODO Find good hyperparameters\n","batch_size = 16\n","learning_rate = 0.0001\n","momentum = 0.95\n","beta1 = momentum\n","beta2 = 0.99\n","num_epochs = 50\n","ith_batch_display = 10000//batch_size\n","\n","\n","print(\"================================================================================================\")\n","print(\"Architectural Change:\")\n","print(\"\\t - doubled Conv Filters\")\n","print(\"\\t - changed to Adam Optimizer \")\n","print(\"\\t - added Residual Shortcuts\")\n","print(\"\\t - added BAtch Norm\")\n","print(\"\\t - changed 1st two Kernel Sizes to 5\")\n","print(\"================================================================================================\")\n","print()\n","print(\"Batch Size:\", batch_size)\n","print(\"learning_rate:\", learning_rate)\n","print(\"beta1:\", beta1)\n","print(\"beta2:\", beta2)\n","print(\"num_epochs:\", num_epochs)\n","print(\"ith_batch_display:\", ith_batch_display)\n","print()\n","\n","# Load train and test data\n","data_loaders = {}\n","data_loaders['train'] = torch.utils.data.DataLoader(\n","                 dataset=train_set,\n","                 batch_size=batch_size,\n","                 shuffle=True)\n","data_loaders['test'] = torch.utils.data.DataLoader(\n","                dataset=test_set,\n","                batch_size=batch_size,\n","                shuffle=False)\n","\n","# implement your own NNs\n","    \n","class MyNeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super(MyNeuralNetwork, self).__init__()\n","        # TODO YOUR CODE HERE\n","        self.n_img = 28*28\n","        self.kernel_size1 = 5\n","        self.padding1 = 2\n","        self.kernel_size = 3\n","        self.kernel_size_sc = 1\n","        self.padding = 1\n","        self.n_ch0 = 1\n","        self.n_ch1 = 64\n","        self.n_ch2 = 64\n","        self.n_ch3 = 128\n","        self.n_ch4 = 128\n","        # self.n_ch5 = 256\n","        # self.n_ch6 = 256\n","        self.n_h1 = 512\n","        self.n_h2 = 10\n","        self.mp_win_size = 2\n","        self.mp_stride = self.mp_win_size\n","        self.p = 0.25\n","        self.conv_sc02 = nn.Conv2d(self.n_ch0, self.n_ch2, self.kernel_size_sc)\n","        self.conv1 = nn.Conv2d(self.n_ch0, self.n_ch1, self.kernel_size1, padding = self.padding1)\n","        self.conv2 = nn.Conv2d(self.n_ch1, self.n_ch2, self.kernel_size1, padding = self.padding1)\n","        self.bn_conv2 = nn.BatchNorm2d(self.n_ch2)\n","        self.maxpool = nn.MaxPool2d(self.mp_win_size, self.mp_stride)\n","        self.dropout = nn.Dropout(self.p)\n","        self.conv_sc24 = nn.Conv2d(self.n_ch2, self.n_ch4, self.kernel_size_sc)\n","        self.conv3 = nn.Conv2d(self.n_ch2, self.n_ch3, self.kernel_size, padding = self.padding)\n","        self.conv4 = nn.Conv2d(self.n_ch3, self.n_ch4, self.kernel_size, padding = self.padding)\n","        self.bn_conv4 = nn.BatchNorm2d(self.n_ch4)\n","        # self.conv_sc46 = nn.Conv2d(self.n_ch4, self.n_ch6, self.kernel_size_sc)\n","        # self.conv5 = nn.Conv2d(self.n_ch4, self.n_ch5, self.kernel_size, padding = self.padding)\n","        # self.conv6 = nn.Conv2d(self.n_ch5, self.n_ch6, self.kernel_size, padding = self.padding)\n","        # self.bn_conv6 = nn.BatchNorm2d(self.n_ch6)\n","        self.fc1 = nn.Linear(6272 , self.n_h1) \n","        self.bn_fc1 = nn.BatchNorm1d(self.n_h1)\n","        self.fc2 = nn.Linear(self.n_h1 , self.n_h2)\n","    def forward(self, x):\n","        # TODO YOUR CODE HERE\n","        residual = self.conv_sc02(x)\n","        x = F.relu(self.conv1(x))\n","        x = self.conv2(x)\n","        x = F.relu(self.bn_conv2(x + residual))\n","        x = self.maxpool(x)\n","        x = self.dropout(x)\n","        residual = self.conv_sc24(x)\n","        x = F.relu(self.conv3(x))\n","        x = self.conv4(x)\n","        x = F.relu(self.bn_conv4(x + residual))\n","        x = self.maxpool(x)\n","        x = self.dropout(x)\n","        # residual = self.conv_sc46(x)\n","        # x = F.relu(self.conv5(x))\n","        # x = F.relu(self.bn_conv6(self.conv6(x) + residual ))\n","        # x = self.maxpool(x)\n","        # x = self.dropout(x)\n","        x = x.view(-1, self.num_flat_features(x)) \n","        x = F.relu(self.bn_fc1(self.fc1(x)))\n","        #x = self.dropout(x)\n","        x = F.softmax(self.fc2(x))\n","        return x\n","    \n","    def num_flat_features(self, x):\n","        size = x.size()[1:]  # all dimensions except the batch dimension\n","        num_features = 1\n","        for s in size:\n","            num_features *= s\n","        return num_features\n","    \n","    def name(self):\n","        return \"MyNeuralNetwork\"\n","\n","\n","\n","## training\n","# model = MyNeuralNetwork()\n","\n","# gpu setup\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model = MyNeuralNetwork().to(device)\n","\n","\n","#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","train_acc_history = []\n","test_acc_history = []\n","\n","train_loss_history = []\n","test_loss_history = []\n","\n","\n","best_acc = 0.0\n","since = time.time()\n","for epoch in range(num_epochs):\n","    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","    print('-' * 10)\n","\n","    # Each epoch has a training and validation phase\n","    for phase in ['train', 'test']:\n","        if phase == 'train':\n","            model.train()  # Set model to training mode\n","        else:\n","            model.eval()  # Set model to evaluate mode\n","\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        for batch_idx, (inputs, labels) in enumerate(data_loaders[phase]):\n","            # ADDON for gpu\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","\n","            \n","            # forward\n","            # track history if only in train\n","            with torch.set_grad_enabled(phase == 'train'):\n","                # ADDON for gpu\n","                outputs = model(inputs)\n","                # outputs = model(inputs)\n","                loss = criterion(outputs, labels)\n","\n","                _, preds = torch.max(outputs, 1)\n","\n","                # backward + optimize only if in training phase\n","                if phase == 'train':\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            # statistics\n","            running_loss += loss.item() * inputs.size(0)\n","            running_corrects += torch.sum(preds == labels.data)\n","\n","            if batch_idx % ith_batch_display == 0:\n","                print('{} Batch: {} of {}'.format(phase, batch_idx, len(data_loaders[phase])))\n","\n","        epoch_loss = running_loss / len(data_loaders[phase].dataset)\n","        epoch_acc = running_corrects.double() / len(data_loaders[phase].dataset)\n","        \n","        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","        # deep copy the model\n","        if phase == 'test' and epoch_acc > best_acc:\n","            best_acc = epoch_acc\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","        if phase == 'test':\n","            test_acc_history.append(epoch_acc)\n","            test_loss_history.append(epoch_loss)\n","        if phase == 'train':\n","            train_acc_history.append(epoch_acc)\n","            train_loss_history.append(epoch_loss)\n","\n","    print()\n","time_elapsed = time.time() - since\n","print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","print('Best val Acc: {:4f}'.format(best_acc))\n","\n","acc_train_hist = []\n","acc_test_hist = []\n","\n","acc_train_hist = [h.cpu().numpy() for h in train_acc_history]\n","acc_test_hist = [h.cpu().numpy() for h in test_acc_history]\n","\n","plt.title(\"Validation/Test Accuracy vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Validation/Test Accuracy\")\n","plt.plot(range(1,num_epochs+1),acc_train_hist,label=\"Train\")\n","plt.plot(range(1,num_epochs+1),acc_test_hist,label=\"Test\")\n","plt.ylim((0,1.))\n","plt.xticks(np.arange(1, num_epochs+1, 1.0))\n","plt.legend()\n","plt.show()\n","\n","\n","examples = enumerate(data_loaders['test'])\n","batch_idx, (example_data, example_targets) = next(examples)\n","with torch.no_grad():\n","    output = model.cpu()(example_data)\n","\n","categories = {\n","    0:\t'T-shirt/top',\n","    1:\t'Trouser',\n","    2:\t'Pullover',\n","    3:\t'Dress',\n","    4:\t'Coat',\n","    5:\t'Sandal',\n","    6:\t'Shirt',\n","    7:\t'Sneaker',\n","    8:\t'Bag',\n","    9:\t'Ankle boot'\n","}\n","\n","for i in range(6):\n","    plt.subplot(2,3,i+1)\n","    plt.tight_layout()\n","    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n","    plt.title(\"Pred: {}\".format(\n","      categories[output.data.max(1, keepdim=True)[1][i].item()]))\n","    plt.xticks([])\n","    plt.yticks([])\n","plt.show()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["PyTorch Version:  1.3.1\n","Torchvision Version:  0.4.2\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["26427392it [00:01, 13862731.53it/s]                             \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 94782.15it/s]                            \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["4423680it [00:01, 4010347.72it/s]                             \n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["8192it [00:00, 33093.51it/s]            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","Processing...\n","Done!\n","Dataset FashionMNIST\n","    Number of datapoints: 60000\n","    Root location: ./data\n","    Split: Train\n","    StandardTransform\n","Transform: Compose(\n","               ToTensor()\n","           )\n","================================================================================================\n","Architectural Change:\n","\t - doubled Conv Filters\n","\t - changed to Adam Optimizer \n","\t - added Residual Shortcuts\n","\t - added BAtch Norm\n","\t - changed 1st two Kernel Sizes to 5\n","================================================================================================\n","\n","Batch Size: 16\n","learning_rate: 0.0001\n","beta1: 0.95\n","beta2: 0.99\n","num_epochs: 50\n","ith_batch_display: 625\n","\n","Epoch 0/49\n","----------\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:134: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["train Batch: 0 of 3750\n","train Batch: 625 of 3750\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9258a01ea0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"W69tUC5qR6U_","colab":{}},"source":["# 15 Epochs with Given Arch: 80.3 in 3:37 min\n","# 15 Epochs with Given Arch + Residual: 78.25 % in 10:40 min\n","# 15 Epochs with Given Arch + Residual + Adam Optimizer: 90.5 % in 05:18 min\n","# 15 Epochs with Given Arch + Residual + Adam Optimizer + BatchNorm: 92.97 % in 11:34 min !!! Not bad !!!\n","# 15 Epochs with Given Arch + Residual + Adam Optimizer + 2 Conv Layers: 90.5 % in 16:35 min\n","# 15 Epochs with 2XFilters + Residual + Adam Optimizer + 2 Conv Layers + BatchNorm: 93.03 % in 21:21 min\n","# 15 Epochs with 2XFilters + Residual + Adam Optimizer + 2 Conv Layers + BatchNorm + Dropout&tanh in FC Layer: 92.3 % in 21:28 min\n","# 15 Epochs with Given Arch + Residual + Adam Optimizer + 2 Conv Layers + BatchNorm + Dropout in FC Layer: 92.1 % in 15:32 min\n","# 15 Epochs with Given Arch + Residual + Adam Optimizer + BatchNorm + Dropout in FC Layer: 92.79 % in 12:21 min\n","\n","# 50 Epochs with Given Arch + Residual + Adam Optimizer + BatchNorm: 94.30 % in 40:39 min\n","# 50 Epochs with 2XFilters + Residual + Adam Optimizer + BatchNorm: 94.12 % in 22.40 min\n","# 15 Epochs with 2XFilters + Residual + Adam Optimizer + BatchNorm + KernelSize=5: 93.21 % in 7.43 min\n","###### ---> 50 Epochs with 2XFilters + Residual + Adam Optimizer + BatchNorm + KernelSize=5: 94.43 % in 25.33 min\n","# 50 Epochs with Given Arch + Residual + Adam Optimizer + BatchNorm + KernelSize=5: 94.00 % in 22:49 min\n","\n","# 50 Epochs with 4XFilters + Residual + Adam Optimizer + BatchNorm + KernelSize=5: 94.16 % in 42.51 min\n","# 50 Epochs with 4XFilters + Residual + Adam Optimizer + BatchNorm + KernelSize=5 + 2 Conv Layers: 94.32 % in 55.41 min\n","\n","# 50 Epochs with 2XFilters + Residual + Adam Optimizer + BatchNorm + KernelSize=5 + 2xNeurons: 94.2 % in 64.19 min\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]}]}