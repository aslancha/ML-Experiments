{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Tumor Segmentation in Brain MRI - GAN - Pix2Pix.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPrj1Lp0odo0sryYkAEc1de"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"myvDi9jemzs-","executionInfo":{"status":"error","timestamp":1619783552154,"user_tz":-480,"elapsed":2758,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"d18da1c9-8215-4abd-95fa-ffa7dda301d7"},"source":["# !apt-get update\n","# !apt-get upgrade\n","!apt-get install openssh-client\n","# !pip install paramiko\n","import paramiko\n","\n","\n","paramiko.util.log_to_file(\"paramiko.log\")\n","\n","# Open a transport\n","host,port = \"sftp://datexis-master2.beuth-hochschule.de\",30064\n","transport = paramiko.Transport((host,port))\n","\n","# Auth    \n","username,password = \"caslan\",\"d1VWntu5cv\"\n","transport.connect(None,username,password)\n","\n","# Go!    \n","sftp = paramiko.SFTPClient.from_transport(transport)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","openssh-client is already the newest version (1:7.6p1-4ubuntu0.3).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded.\n"],"name":"stdout"},{"output_type":"error","ename":"gaierror","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-0f3ef8fc4b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Open a transport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sftp://datexis-master2.beuth-hochschule.de\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30064\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtransport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparamiko\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Auth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/paramiko/transport.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock, default_window_size, default_max_packet_size, gss_kex, gss_deleg_creds, disabled_algorithms)\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No suitable address family\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             addrinfos = socket.getaddrinfo(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0mhostname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNSPEC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             )\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msockaddr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maddrinfos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;31m# and socket type values to enum constants.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         addrlist.append((_intenum_converter(af, AddressFamily),\n","\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known"]}]},{"cell_type":"code","metadata":{"id":"nY8ijin5RgXe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619079562567,"user_tz":-480,"elapsed":6623,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"5604468d-7b42-4ae6-c319-a524cb50711c"},"source":["# !pip install git+https://github.com/albumentations-team/albumentations.git\n","!pip install --upgrade --force-reinstall --no-deps albumentations\n","!pip install pydicom"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting albumentations\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n","\r\u001b[K     |████▌                           | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20kB 31.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 30kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 40kB 25.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 51kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 61kB 26.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 71kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n","\u001b[?25hInstalling collected packages: albumentations\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.5.2\n","Collecting pydicom\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 17.4MB/s \n","\u001b[?25hInstalling collected packages: pydicom\n","Successfully installed pydicom-2.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0E5oIRg0zdjI","executionInfo":{"status":"ok","timestamp":1619855487581,"user_tz":-480,"elapsed":4007,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"7a85f4f5-a17d-45e5-fbca-e1d0ec7c9a90","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip show torch"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Name: torch\n","Version: 1.8.1+cu101\n","Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n","Home-page: https://pytorch.org/\n","Author: PyTorch Team\n","Author-email: packages@pytorch.org\n","License: BSD-3\n","Location: /usr/local/lib/python3.7/dist-packages\n","Requires: typing-extensions, numpy\n","Required-by: torchvision, torchtext, fastai\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iEMrG7XJGvsQ"},"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","from PIL import Image\n","import numpy as np\n","import os\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.utils import save_image\n","from tqdm import tqdm\n","\n","\n","DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","LEARNING_RATE = 2e-4\n","BATCH_SIZE = 16\n","NUM_WORKERS = 2\n","IMG_SIZE = 256\n","CH_IMG = 3\n","L1_LAMBDA = 100\n","NUM_EPOCHS = 500\n","LOAD_MODEL = False\n","SAVE_MODEL = True\n","CHECKPOINT_DISC = \"disc.pth.tar\"\n","CHECKPOINT_GEN = \"gen.pth.tar\"\n","TRAIN_PATH = \"/content/maps/train\"\n","VAL_PATH = \"/content/maps/val\"\n","EVAL_PATH = \"/content/maps/evaluation\"\n","\n","both_transform = A.Compose(\n","    [A.Resize(width=256, height=256)], additional_targets={\"image0\": \"image\"}\n",")\n","\n","transform_only_input = A.Compose(\n","    [\n","        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n","        A.pytorch.transforms.ToTensor(),\n","    ]\n",")\n","\n","transform_only_mask = A.Compose(\n","    [\n","        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0),\n","        A.pytorch.transforms.ToTensor(),\n","    ]\n",")\n","\n","class MapDataset(Dataset):\n","    def __init__(self, root_dir):\n","        self.root_dir = root_dir\n","        self.list_files = os.listdir(self.root_dir)\n","\n","    def __len__(self):\n","        return len(self.list_files)\n","    \n","    def __getitem__(self, index):\n","        img_name = self.list_files[index]\n","        img_path = os.path.join(self.root_dir, img_name)\n","        print(img_path)\n","        image = np.array(Image.open(img_path))\n","        input_image = image[:, :600, :]\n","        target_image = image[:, 600:, :]\n","\n","        augmentations = both_transform(image=input_image, image0=target_image)\n","        input_image, target_image = augmentations[\"image\"], augmentations[\"image0\"]\n","\n","        input_image = transform_only_input(image=input_image)[\"image\"]\n","        target_image = transform_only_mask(image=target_image)[\"image\"]\n","        return input_image, target_image\n","\n","def save_some_examples(gen, val_loader, epoch, folder):\n","    x, y = next(iter(val_loader))\n","    x, y = x.to(DEVICE), y.to(DEVICE)\n","    gen.eval()\n","    if not os.path.exists(folder): os.mkdir(folder) \n","    with torch.no_grad():\n","        y_fake = gen(x)\n","        y_fake = y_fake * 0.5 + 0.5\n","        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n","        save_image(x*0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n","        if epoch == 1:\n","            save_image(y*0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n","    gen.train()\n","\n","def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n","    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n","    torch.save(checkpoint, filename)\n","def load_checkpoint(checkpoint_file, model, optimizer, lr):\n","    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","\n","    for param_group in optimizer.param_groups:\n","        param_groups[\"lr\"] = lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3ElF9jKRW6N","executionInfo":{"status":"ok","timestamp":1618922119693,"user_tz":-480,"elapsed":2325,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"3bd92fb2-608c-49ca-9604-c52c02865559"},"source":["import torch.nn as nn\n","\n","class CNNBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=2):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, bias=False, padding_mode=\"reflect\"),\n","            nn.BatchNorm2d(out_channels),\n","            nn.LeakyReLU(0.2),\n","        )\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n","        super().__init__()\n","        self.initial = nn.Sequential(\n","            nn.Conv2d(in_channels*2, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n","            nn.LeakyReLU(0.2),\n","        )\n","        layers = []\n","        in_channels = features[0]\n","        for feature in features[1:]:\n","            layers.append(\n","                CNNBlock(in_channels, feature, stride=1 if feature == features[-1] else 2)\n","            )\n","            in_channels = feature\n","        layers.append(\n","            nn.Conv2d(\n","                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n","            )\n","        )\n","        self.model = nn.Sequential(*layers)\n","    \n","    def forward(self, x, y):\n","        x = torch.cat([x,y], dim=1)\n","        x = self.initial(x)\n","        return self.model(x)\n","\n","def test_Disc():\n","    x = torch.randn((1, 3, 286, 286))\n","    y = torch.randn((1, 3, 286, 286))\n","    model = Discriminator()\n","    preds = model(x, y)\n","    print(preds.shape)\n","\n","test_Disc()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 1, 30, 30])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQqi3nfZyWqC","executionInfo":{"status":"ok","timestamp":1618922120398,"user_tz":-480,"elapsed":3021,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"3f1a4d7d-3eb2-4574-e54c-e71780bb6524"},"source":["class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn. Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n","            if down\n","            else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU() if act==\"relu\" else nn.LeakyReLU(0.2),\n","        )\n","        self.use_dropout = use_dropout\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        return self.dropout(x) if self.use_dropout else x\n","\n","class Generator(nn.Module):\n","    def __init__(self, in_channels=3, features=64):\n","        super().__init__()\n","        self.initial_down = nn.Sequential(\n","            nn.Conv2d(in_channels, features, 4, 2, 1, padding_mode=\"reflect\"),\n","            nn.LeakyReLU(0.2),\n","        ) # 128\n","        self.down1 = Block(features, features*2, down=True, act=\"Leaky\", use_dropout=False) # 64\n","        self.down2 = Block(features*2, features*4, down=True, act=\"Leaky\", use_dropout=False) # 32\n","        self.down3 = Block(features*4, features*8, down=True, act=\"Leaky\", use_dropout=False) # 16\n","        self.down4 = Block(features*8, features*8, down=True, act=\"Leaky\", use_dropout=False) # 8\n","        self.down5 = Block(features*8, features*8, down=True, act=\"Leaky\", use_dropout=False) # 4\n","        self.down6 = Block(features*8, features*8, down=True, act=\"Leaky\", use_dropout=False) # 2\n","        self.bottleneck = nn.Sequential(\n","            nn.Conv2d(features*8, features*8, 4, 2, 1), nn.ReLU(), # 1x1\n","        )\n","        self.up1 = Block(features*8, features*8, down=False, act=\"relu\", use_dropout=True)\n","        self.up2 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n","        self.up3 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=True)\n","        self.up4 = Block(features*8*2, features*8, down=False, act=\"relu\", use_dropout=False)\n","        self.up5 = Block(features*8*2, features*4, down=False, act=\"relu\", use_dropout=False)\n","        self.up6 = Block(features*4*2, features*2, down=False, act=\"relu\", use_dropout=False)\n","        self.up7 = Block(features*2*2, features, down=False, act=\"relu\", use_dropout=False)\n","        self.final_up = nn.Sequential(\n","            nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n","            nn.Tanh(),\n","        )\n","\n","    def forward(self, x):\n","        d1 = self.initial_down(x)\n","        d2 = self.down1(d1)\n","        d3 = self.down2(d2)\n","        d4 = self.down3(d3)\n","        d5 = self.down4(d4)\n","        d6 = self.down5(d5)\n","        d7 = self.down6(d6)\n","        bottleneck = self.bottleneck(d7)\n","        up1 = self.up1(bottleneck)\n","        up2 = self.up2(torch.cat([up1, d7], 1))\n","        up3 = self.up3(torch.cat([up2, d6], 1))\n","        up4 = self.up4(torch.cat([up3, d5], 1))\n","        up5 = self.up5(torch.cat([up4, d4], 1))\n","        up6 = self.up6(torch.cat([up5, d3], 1))\n","        up7 = self.up7(torch.cat([up6, d2], 1))\n","        return self.final_up(torch.cat([up7, d1], 1))\n","\n","def test_gen():\n","     x = torch.randn((1, 3, 256, 256))\n","     model = Generator(in_channels=3, features=64)    \n","     preds = model(x)\n","     print(preds.shape)    \n","\n","test_gen()\n","    \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 3, 256, 256])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XZXddBPktIx3"},"source":["import torch.optim as optim\n","\n","def train_fn(disc, gen, loader, opt_disc, opt_gen, l1, bce, g_scaler, d_scaler):\n","    loop = tqdm(loader, leave=True)\n","\n","    for idx, (x, y) in enumerate(loop):\n","        x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","        # Train Discriminator\n","        with torch.cuda.amp.autocast(): # float16 training for less VRAM\n","            y_fake = gen(x)\n","            D_real = disc(x, y)\n","            D_fake = disc(x, y_fake.detach())\n","            D_real_loss = bce(D_real, torch.ones_like(D_real))\n","            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n","            D_loss = (D_real_loss + D_fake_loss) / 2\n","\n","        disc.zero_grad()\n","        d_scaler.scale(D_loss).backward()\n","        d_scaler.step(opt_disc)\n","        d_scaler.update()\n","\n","        # Train Generator\n","        with torch.cuda.amp.autocast():\n","            D_fake = disc(x, y_fake)\n","            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n","            L1 = l1(y_fake, y) * L1_LAMBDA\n","            G_loss = G_fake_loss + L1\n","\n","        gen.zero_grad()\n","        g_scaler.scale(G_loss).backward()\n","        g_scaler.step(opt_gen)\n","        g_scaler.update()\n","\n","def main():\n","    disc = Discriminator(in_channels=3).to(DEVICE)\n","    gen = Generator(in_channels=3). to(DEVICE)\n","    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n","    BCE = nn.BCEWithLogitsLoss()\n","    L1_LOSS = nn.L1Loss()\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(CHECKPOINT_GEN, gen, opt_gen, LEARNING_RATE)\n","        load_checkpoint(CHECKPOINT_DISC, disc, opt_disc, LEARNING_RATE)\n","\n","    train_dataset = MapDataset(root_dir=TRAIN_PATH)\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n","    g_scaler = torch.cuda.amp.GradScaler() # float16 training for less VRAM\n","    d_scaler = torch.cuda.amp.GradScaler()\n","    val_dataset = MapDataset(root_dir=VAL_PATH)\n","    val_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n","    \n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(disc, gen, train_loader, opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n","        \n","        if SAVE_MODEL and epoch % 5 == 0:\n","            save_checkpoint(gen, opt_gen, filename=CHECKPOINT_GEN) \n","            save_checkpoint(disc, opt_disc, filename=CHECKPOINT_DISC)\n","        save_some_examples(gen, val_loader, epoch, folder=EVAL_PATH)     "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"95GXxb0mO2QE"},"source":["import tarfile\n","import urllib.request\n","import os\n","\n","if not os.path.exists(\"/content/\"):\n","    #\"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/maps.tar.gz\"\n","    urllib.request.urlretrieve(\"https://drive.google.com/uc?export=download&id=1i-enk8S4LOr_7HGLXjB24sI5nHU08NUm/\", \"data.tar.gz\")\n","\n","    tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n","    tar.extractall()\n","    tar.close()\n","# main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"id":"BjHyTBsZqhrx","executionInfo":{"status":"error","timestamp":1619080348696,"user_tz":-480,"elapsed":922,"user":{"displayName":"C. A.","photoUrl":"","userId":"13925713632893850583"}},"outputId":"a2adec07-6f8d-4634-d748-138e3bad8451"},"source":["# authors : Guillaume Lemaitre <g.lemaitre58@gmail.com>\n","# license : MIT\n","\n","import matplotlib.pyplot as plt\n","from pydicom import dcmread\n","from pydicom.data import get_testdata_file\n","\n","fpath = get_testdata_file('./data/BrainMRI/DSC/1-0001.dcm')\n","ds = dcmread(fpath)\n","\n","# Normal mode:\n","print()\n","print(f\"File path........: {fpath}\")\n","print(f\"SOP Class........: {ds.SOPClassUID} ({ds.SOPClassUID.name})\")\n","print()\n","\n","pat_name = ds.PatientName\n","display_name = pat_name.family_name + \", \" + pat_name.given_name\n","print(f\"Patient's Name...: {display_name}\")\n","print(f\"Patient ID.......: {ds.PatientID}\")\n","print(f\"Modality.........: {ds.Modality}\")\n","print(f\"Study Date.......: {ds.StudyDate}\")\n","print(f\"Image size.......: {ds.Rows} x {ds.Columns}\")\n","print(f\"Pixel Spacing....: {ds.PixelSpacing}\")\n","\n","# use .get() if not sure the item exists, and want a default value if missing\n","print(f\"Slice location...: {ds.get('SliceLocation', '(missing)')}\")\n","\n","# plot the image using matplotlib\n","plt.imshow(ds.pixel_array, cmap=plt.cm.gray)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-f7c6be1d379b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_testdata_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/BrainMRI/DSC/1-0001.dcm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcmread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Normal mode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pydicom/filereader.py\u001b[0m in \u001b[0;36mdcmread\u001b[0;34m(fp, defer_size, stop_before_pixels, force, specific_tags)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m         raise TypeError(\"dcmread: Expected a file path or a file-like, \"\n\u001b[0;32m--> 864\u001b[0;31m                         \"but got \" + type(fp).__name__)\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: dcmread: Expected a file path or a file-like, but got NoneType"]}]},{"cell_type":"code","metadata":{"id":"RETT-RiWtOqJ"},"source":[""],"execution_count":null,"outputs":[]}]}